{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to export model to ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:626: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if channels < 1 or channels > 2:\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:643: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_length % stride) - step != 0:\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:148: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_pad = max(padding_left, padding_right)\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if length <= max_pad:\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4476: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved to ../public/encodec_24khz.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EncodecModel, AutoProcessor, EncodecConfig\n",
    "\n",
    "# Subclass EncodecModel to modify the forward method\n",
    "class CustomEncodecModel(EncodecModel):\n",
    "    def forward(self, input_values: torch.Tensor,\n",
    "                padding_mask=None, bandwidth=None, audio_codes=None,\n",
    "                audio_scales=None, return_dict=True):\n",
    "        # Perform encoding only (no decoding)\n",
    "        audio_codes, audio_scales = self.encode(input_values, padding_mask, bandwidth, False)\n",
    "        return audio_codes  # Return only the encoded audio codes\n",
    "\n",
    "def convert_encodec_to_onnx(model_name=\"facebook/encodec_24khz\", output_path=\"../public/encodec_24khz.onnx\"):\n",
    "    # Load the custom Encodec model\n",
    "    model = CustomEncodecModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # Create dummy inputs with dynamic axes\n",
    "    dummy_input = torch.randn(1, 1, 24000, dtype=torch.float32)  # 1 second of audio (for batch size 1)\n",
    "    dummy_padding_mask = torch.zeros(1, 24000, dtype=torch.bool)  # Padding mask for sequence length\n",
    "\n",
    "    print('Preparing to export model to ONNX') \n",
    "\n",
    "    # Export the model with dynamic batch size and sequence length\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input, dummy_padding_mask),\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=16,\n",
    "        do_constant_folding=True,  # Apply constant folding optimization\n",
    "        input_names=[\"input_values\", \"padding_mask\",'bandwidth'],\n",
    "        output_names=[\"audio_codes\"],  # Only output the encoded audio codes\n",
    "        dynamic_axes={\n",
    "            \"input_values\": {0: \"batch_size\", 2: \"sequence_length\"},  # Dynamic axes for input\n",
    "            \"padding_mask\": {0: \"batch_size\", 1: \"sequence_length\"},  # Dynamic axes for padding mask\n",
    "            \"audio_codes\": {0: \"batch_size\"}  # Dynamic batch size for output\n",
    "        }\n",
    "    )\n",
    "    print(f\"ONNX model saved to {output_path}\")\n",
    "\n",
    "# Usage\n",
    "convert_encodec_to_onnx()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
